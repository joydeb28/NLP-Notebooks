{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nimport transformers\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import BertForTokenClassification, BertTokenizer, BertConfig, BertModel\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-23T16:35:42.782585Z","iopub.execute_input":"2021-05-23T16:35:42.782912Z","iopub.status.idle":"2021-05-23T16:35:42.788124Z","shell.execute_reply.started":"2021-05-23T16:35:42.782881Z","shell.execute_reply":"2021-05-23T16:35:42.787007Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"from torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2021-05-23T16:35:42.789807Z","iopub.execute_input":"2021-05-23T16:35:42.790383Z","iopub.status.idle":"2021-05-23T16:35:42.799571Z","shell.execute_reply.started":"2021-05-23T16:35:42.790349Z","shell.execute_reply":"2021-05-23T16:35:42.798911Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/entity-annotated-corpus/ner.csv\", encoding = \"ISO-8859-1\", error_bad_lines=False)\ndataset = df[['pos','sentence_idx','word','tag']]\ndataset.head(50)\ndataset = dataset.drop_duplicates()\n","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-05-23T16:35:42.801109Z","iopub.execute_input":"2021-05-23T16:35:42.801793Z","iopub.status.idle":"2021-05-23T16:35:47.485178Z","shell.execute_reply.started":"2021-05-23T16:35:42.801758Z","shell.execute_reply":"2021-05-23T16:35:47.484375Z"},"trusted":true},"execution_count":117,"outputs":[{"name":"stderr","text":"b'Skipping line 281837: expected 25 fields, saw 34\\n'\n","output_type":"stream"}]},{"cell_type":"code","source":"class SentenceGetter(object):\n    \n    def __init__(self, dataset):\n        self.n_sent = 1\n        self.dataset = dataset\n        self.empty = False\n        agg_func = lambda s: [(w,p, t) for w,p, t in zip(s[\"word\"].values.tolist(),\n                                                       s['pos'].values.tolist(),\n                                                        s[\"tag\"].values.tolist())]\n        self.grouped = self.dataset.groupby(\"sentence_idx\").apply(agg_func)\n        self.sentences = [s for s in self.grouped]\n    \n    def get_next(self):\n        try:\n            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n            self.n_sent += 1\n            return s\n        except:\n            return None\n\ngetter = SentenceGetter(dataset)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T16:35:47.487613Z","iopub.execute_input":"2021-05-23T16:35:47.487962Z","iopub.status.idle":"2021-05-23T16:35:50.836843Z","shell.execute_reply.started":"2021-05-23T16:35:47.487926Z","shell.execute_reply":"2021-05-23T16:35:50.835942Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"print(len(getter.sentences[:1000]))\nconsidered_len = 1000","metadata":{"execution":{"iopub.status.busy":"2021-05-23T16:35:50.839311Z","iopub.execute_input":"2021-05-23T16:35:50.839679Z","iopub.status.idle":"2021-05-23T16:35:50.844101Z","shell.execute_reply.started":"2021-05-23T16:35:50.839644Z","shell.execute_reply":"2021-05-23T16:35:50.843199Z"},"trusted":true},"execution_count":119,"outputs":[{"name":"stdout","text":"1000\n","output_type":"stream"}]},{"cell_type":"code","source":"\ntags_vals = list(set(dataset[\"tag\"].values))\ntag2idx = {t: i for i, t in enumerate(tags_vals)}\nsentences = [' '.join([s[0] for s in sent]) for sent in getter.sentences]\nlabels = [[s[2] for s in sent] for sent in getter.sentences]\nlabels = [[tag2idx.get(l) for l in lab] for lab in labels]","metadata":{"execution":{"iopub.status.busy":"2021-05-23T16:35:50.845671Z","iopub.execute_input":"2021-05-23T16:35:50.846228Z","iopub.status.idle":"2021-05-23T16:35:51.456526Z","shell.execute_reply.started":"2021-05-23T16:35:50.846134Z","shell.execute_reply":"2021-05-23T16:35:51.455622Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 200\nTRAIN_BATCH_SIZE = 32\nVALID_BATCH_SIZE = 16\nEPOCHS = 5\nLEARNING_RATE = 2e-05\ntokenizer = BertTokenizer.from_pretrained('distilbert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2021-05-23T16:35:51.459082Z","iopub.execute_input":"2021-05-23T16:35:51.459731Z","iopub.status.idle":"2021-05-23T16:35:51.850229Z","shell.execute_reply.started":"2021-05-23T16:35:51.459692Z","shell.execute_reply":"2021-05-23T16:35:51.849173Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, tokenizer, sentences, labels, max_len):\n        self.len = len(sentences)\n        self.sentences = sentences\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __getitem__(self, index):\n        sentence = str(self.sentences[index])\n        inputs = self.tokenizer.encode_plus(\n            sentence,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n            return_token_type_ids=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        label = self.labels[index]\n        label.extend([4]*200)\n        label=label[:200]\n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'tags': torch.tensor(label, dtype=torch.long)\n        } \n    \n    def __len__(self):\n        return self.len","metadata":{"execution":{"iopub.status.busy":"2021-05-23T16:35:51.851713Z","iopub.execute_input":"2021-05-23T16:35:51.852082Z","iopub.status.idle":"2021-05-23T16:35:51.862566Z","shell.execute_reply.started":"2021-05-23T16:35:51.852045Z","shell.execute_reply":"2021-05-23T16:35:51.860922Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"# Creating the dataset and dataloader for the neural network\n\ntrain_percent = 0.8\ntrain_size = int(train_percent*len(sentences))\n# train_dataset=df.sample(frac=train_size,random_state=200).reset_index(drop=True)\n# test_dataset=df.drop(train_dataset.index).reset_index(drop=True)\ntrain_sentences = sentences[0:train_size]\ntrain_labels = labels[0:train_size]\n\ntest_sentences = sentences[train_size:]\ntest_labels = labels[train_size:]\n\nprint(\"FULL Dataset: {}\".format(len(sentences)))\nprint(\"TRAIN Dataset: {}\".format(len(train_sentences)))\nprint(\"TEST Dataset: {}\".format(len(test_sentences)))\n\ntraining_set = CustomDataset(tokenizer, train_sentences, train_labels, MAX_LEN)\ntesting_set = CustomDataset(tokenizer, test_sentences, test_labels, MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T16:35:51.865021Z","iopub.execute_input":"2021-05-23T16:35:51.865447Z","iopub.status.idle":"2021-05-23T16:35:51.880006Z","shell.execute_reply.started":"2021-05-23T16:35:51.865407Z","shell.execute_reply":"2021-05-23T16:35:51.879032Z"},"trusted":true},"execution_count":123,"outputs":[{"name":"stdout","text":"FULL Dataset: 35177\nTRAIN Dataset: 28141\nTEST Dataset: 7036\n","output_type":"stream"}]},{"cell_type":"code","source":"train_params = {'batch_size': TRAIN_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntest_params = {'batch_size': VALID_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntraining_loader = DataLoader(training_set, **train_params)\ntesting_loader = DataLoader(testing_set, **test_params)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T16:35:51.881250Z","iopub.execute_input":"2021-05-23T16:35:51.883688Z","iopub.status.idle":"2021-05-23T16:35:51.994024Z","shell.execute_reply.started":"2021-05-23T16:35:51.883659Z","shell.execute_reply":"2021-05-23T16:35:51.992979Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"class BERTClass(torch.nn.Module):\n    def __init__(self):\n        super(BERTClass, self).__init__()\n        self.l1 = transformers.BertForTokenClassification.from_pretrained('distilbert-base-uncased', num_labels=18)\n        # self.l2 = torch.nn.Dropout(0.3)\n        # self.l3 = torch.nn.Linear(768, 200)\n    \n    def forward(self, ids, mask, labels):\n        output_1= self.l1(ids, mask, labels = labels)\n        # output_2 = self.l2(output_1[0])\n        # output = self.l3(output_2)\n        return output_1","metadata":{"execution":{"iopub.status.busy":"2021-05-23T16:35:51.995518Z","iopub.execute_input":"2021-05-23T16:35:51.996155Z","iopub.status.idle":"2021-05-23T16:35:52.007968Z","shell.execute_reply.started":"2021-05-23T16:35:51.996109Z","shell.execute_reply":"2021-05-23T16:35:52.007277Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"model = BERTClass()\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T16:35:52.009670Z","iopub.execute_input":"2021-05-23T16:35:52.010149Z","iopub.status.idle":"2021-05-23T16:35:55.469503Z","shell.execute_reply.started":"2021-05-23T16:35:52.010083Z","shell.execute_reply":"2021-05-23T16:35:55.468395Z"},"trusted":true},"execution_count":126,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertForTokenClassification: ['distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":126,"output_type":"execute_result","data":{"text/plain":"BERTClass(\n  (l1): BertForTokenClassification(\n    (bert): BertModel(\n      (embeddings): BertEmbeddings(\n        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n        (position_embeddings): Embedding(512, 768)\n        (token_type_embeddings): Embedding(2, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder): BertEncoder(\n        (layer): ModuleList(\n          (0): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (1): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (2): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (3): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (4): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (5): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (6): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (7): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (8): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (9): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (10): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (11): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n    )\n    (dropout): Dropout(p=0.1, inplace=False)\n    (classifier): Linear(in_features=768, out_features=18, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T16:35:55.470913Z","iopub.execute_input":"2021-05-23T16:35:55.471280Z","iopub.status.idle":"2021-05-23T16:35:55.479538Z","shell.execute_reply.started":"2021-05-23T16:35:55.471245Z","shell.execute_reply":"2021-05-23T16:35:55.478669Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"#from seqeval.metrics import f1_score\nfrom sklearn.metrics import f1_score\ndef flat_accuracy(preds, labels):\n    flat_preds = np.argmax(preds, axis=2).flatten()\n    flat_labels = labels.flatten()\n    return np.sum(flat_preds == flat_labels)/len(flat_labels)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T16:35:55.481249Z","iopub.execute_input":"2021-05-23T16:35:55.481747Z","iopub.status.idle":"2021-05-23T16:35:55.491946Z","shell.execute_reply.started":"2021-05-23T16:35:55.481708Z","shell.execute_reply":"2021-05-23T16:35:55.491290Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"def train_eval(epoch):\n    model.train()\n    train_loss = 0\n    total_loss = 0\n    train_accuracy = 0\n    nb_train_steps, nb_train_examples = 0, 0\n    predictions , true_labels = [], []\n    for _,data in enumerate(training_loader, 0):\n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        targets = data['tags'].to(device, dtype = torch.long)\n\n        #loss = model(ids, mask, labels = targets)[0]\n        #total_loss +=loss.item()\n        # Accuracy\n        output = model(ids, mask, labels=targets)\n        #print(output)\n        loss = output[0]\n        logits = output[1]\n        logits = logits.detach().cpu().numpy()\n        label_ids = targets.to('cpu').numpy()\n        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n        true_labels.append(label_ids)\n        accuracy = flat_accuracy(logits, label_ids)\n        train_loss += loss.mean().item()\n        train_accuracy += accuracy\n        nb_train_examples += ids.size(0)\n        nb_train_steps += 1\n        train_loss = train_loss/nb_train_steps\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    print(f\"****** Epoch {epoch+1} ******\")\n    print(f\"\\nTraining loss: {train_loss}\")\n    print(f\"Training Accuracy: {train_accuracy/nb_train_steps}\")\n    pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n    actual_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n    print(\"Training F1-Score: {}\".format(f1_score(actual_tags, pred_tags, average='weighted')))\n    \n    model.eval()\n    eval_loss = 0; eval_accuracy = 0\n    n_correct = 0; n_wrong = 0; total = 0\n    predictions , true_labels = [], []\n    nb_eval_steps, nb_eval_examples = 0, 0\n    with torch.no_grad():\n        for _, data in enumerate(testing_loader, 0):\n            ids = data['ids'].to(device, dtype = torch.long)\n            mask = data['mask'].to(device, dtype = torch.long)\n            targets = data['tags'].to(device, dtype = torch.long)\n\n            output = model(ids, mask, labels=targets)\n            #print(output)\n            #loss, logits = output[:2]\n            loss = output[0]\n            logits = output[1]\n            logits = logits.detach().cpu().numpy()\n            label_ids = targets.to('cpu').numpy()\n            predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n            true_labels.append(label_ids)\n            accuracy = flat_accuracy(logits, label_ids)\n            eval_loss += loss.mean().item()\n            eval_accuracy += accuracy\n            nb_eval_examples += ids.size(0)\n            nb_eval_steps += 1\n        eval_loss = eval_loss/nb_eval_steps\n        print(\"\\nValidation loss: {}\".format(eval_loss))\n        print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n        pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n        valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n        print(\"Validation F1-Score: {}\".format(f1_score(valid_tags, pred_tags,average='weighted')))","metadata":{"execution":{"iopub.status.busy":"2021-05-23T16:35:55.495190Z","iopub.execute_input":"2021-05-23T16:35:55.495426Z","iopub.status.idle":"2021-05-23T16:35:55.526119Z","shell.execute_reply.started":"2021-05-23T16:35:55.495404Z","shell.execute_reply":"2021-05-23T16:35:55.525361Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"for epoch in range(2):\n    train_eval(epoch)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T16:35:55.528985Z","iopub.execute_input":"2021-05-23T16:35:55.529289Z","iopub.status.idle":"2021-05-23T16:57:20.484141Z","shell.execute_reply.started":"2021-05-23T16:35:55.529244Z","shell.execute_reply":"2021-05-23T16:57:20.482704Z"},"trusted":true},"execution_count":130,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"****** Epoch 1 ******\n\nTraining loss: 0.0007159462428962515\nTraining Accuracy: 0.6656405293924825\nTraining F1-Score: 0.7345428306115956\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","output_type":"stream"},{"name":"stdout","text":"\nValidation loss: 0.567285154895349\nValidation Accuracy: 0.8196647727272726\nValidation F1-Score: 0.851524375992452\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","output_type":"stream"},{"name":"stdout","text":"****** Epoch 2 ******\n\nTraining loss: 0.0006558471907217491\nTraining Accuracy: 0.7902754179414334\nTraining F1-Score: 0.8312478430310759\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","output_type":"stream"},{"name":"stdout","text":"\nValidation loss: 0.37828629247166895\nValidation Accuracy: 0.8554299242424243\nValidation F1-Score: 0.8798236313123742\n","output_type":"stream"}]},{"cell_type":"code","source":"def valid(model, testing_loader):\n    model.eval()\n    eval_loss = 0; eval_accuracy = 0\n    n_correct = 0; n_wrong = 0; total = 0\n    predictions , true_labels = [], []\n    nb_eval_steps, nb_eval_examples = 0, 0\n    with torch.no_grad():\n        for _, data in enumerate(testing_loader, 0):\n            ids = data['ids'].to(device, dtype = torch.long)\n            mask = data['mask'].to(device, dtype = torch.long)\n            targets = data['tags'].to(device, dtype = torch.long)\n\n            output = model(ids, mask, labels=targets)\n            #print(output)\n            #loss, logits = output[:2]\n            loss = output[0]\n            logits = output[1]\n            logits = logits.detach().cpu().numpy()\n            label_ids = targets.to('cpu').numpy()\n            predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n            true_labels.append(label_ids)\n            accuracy = flat_accuracy(logits, label_ids)\n            eval_loss += loss.mean().item()\n            eval_accuracy += accuracy\n            nb_eval_examples += ids.size(0)\n            nb_eval_steps += 1\n        eval_loss = eval_loss/nb_eval_steps\n        print(\"Validation loss: {}\".format(eval_loss))\n        print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n        pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n        valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n        print(\"F1-Score: {}\".format(f1_score(pred_tags, valid_tags,average='weighted')))","metadata":{"execution":{"iopub.status.busy":"2021-05-23T16:57:20.487977Z","iopub.execute_input":"2021-05-23T16:57:20.488264Z","iopub.status.idle":"2021-05-23T16:57:20.499238Z","shell.execute_reply.started":"2021-05-23T16:57:20.488236Z","shell.execute_reply":"2021-05-23T16:57:20.498126Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":"# valid(model, testing_loader)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T16:57:20.500779Z","iopub.execute_input":"2021-05-23T16:57:20.501201Z","iopub.status.idle":"2021-05-23T16:57:20.518054Z","shell.execute_reply.started":"2021-05-23T16:57:20.501165Z","shell.execute_reply":"2021-05-23T16:57:20.517395Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"torch.save(model,\"ner_bert.pt\")","metadata":{"execution":{"iopub.status.busy":"2021-05-23T16:57:20.522066Z","iopub.execute_input":"2021-05-23T16:57:20.522442Z","iopub.status.idle":"2021-05-23T16:57:21.775349Z","shell.execute_reply.started":"2021-05-23T16:57:20.522419Z","shell.execute_reply":"2021-05-23T16:57:21.774229Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"class Prediction():\n    \n    def __init__(self,tokenizer,model):\n        self.device = \"cpu\"\n        self.model = model.to(self.device)\n        self.tokenizer = tokenizer\n        \n    def predict_(self,sentences):\n#         tokenized_sentence = tokenizer.encode(sentences)\n#         inputs = torch.tensor(tokenized_sentence).cuda()\n        \n        ''''''\n        inputs = self.tokenizer.encode_plus(\n            sentences,\n            None,\n            add_special_tokens=True,\n            max_length=200,\n            pad_to_max_length=True,\n            return_token_type_ids=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        label = []\n        label.extend([4]*200)\n        label=label[:200]\n\n        ids = torch.tensor(ids, dtype=torch.long)\n        ids = torch.reshape(ids,(1,ids.shape[0]))\n        print(ids.shape)\n        mask = torch.tensor(mask, dtype=torch.long)\n        mask = torch.reshape(mask,(1,mask.shape[0]))\n        labels = torch.tensor(label, dtype=torch.long)\n        labels = torch.reshape(labels,(1,labels.shape[0]))\n        with torch.no_grad():\n            output = self.model(ids,mask,labels)\n        logits = output[1]\n        logits = logits.detach().cpu().numpy()\n        preds = list(p for p in np.argmax(logits, axis=2))\n        labels = []\n        for item in preds[0]:\n            labels.append(tags_vals[item])\n        \n        \n        tokens = self.tokenizer.convert_ids_to_tokens(ids.to('cpu').numpy()[0])\n        new_tokens, new_labels = [], []\n        for token, label_idx in zip(tokens[1:], preds[0]):\n            if token == \"[SEP]\":\n                break\n            if token.startswith(\"##\"):\n                new_tokens[-1] = new_tokens[-1] + token[2:]\n            else:\n                new_labels.append(tags_vals[label_idx])\n                new_tokens.append(token)\n        for token, label in zip(new_tokens, new_labels):\n            print(\"{}\\t{}\".format(label, token))\n        return labels","metadata":{"execution":{"iopub.status.busy":"2021-05-23T16:57:21.777174Z","iopub.execute_input":"2021-05-23T16:57:21.777708Z","iopub.status.idle":"2021-05-23T16:57:21.800326Z","shell.execute_reply.started":"2021-05-23T16:57:21.777665Z","shell.execute_reply":"2021-05-23T16:57:21.799111Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"code","source":"pred = Prediction(tokenizer,model)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T16:57:21.801968Z","iopub.execute_input":"2021-05-23T16:57:21.802355Z","iopub.status.idle":"2021-05-23T16:57:22.469137Z","shell.execute_reply.started":"2021-05-23T16:57:21.802319Z","shell.execute_reply":"2021-05-23T16:57:22.468315Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"test_sentences[0]","metadata":{"execution":{"iopub.status.busy":"2021-05-23T16:57:22.470403Z","iopub.execute_input":"2021-05-23T16:57:22.470741Z","iopub.status.idle":"2021-05-23T16:57:22.476962Z","shell.execute_reply.started":"2021-05-23T16:57:22.470706Z","shell.execute_reply":"2021-05-23T16:57:22.476124Z"},"trusted":true},"execution_count":136,"outputs":[{"execution_count":136,"output_type":"execute_result","data":{"text/plain":"'President Yushchenko and Tymoshenko were once allies , but their relationship deteriorated because of bitter political infighting a year after the 2004 Orange Revolution that swept Mr. Yushchenko to power .'"},"metadata":{}}]},{"cell_type":"code","source":"res = pred.predict_(test_sentences[0])","metadata":{"execution":{"iopub.status.busy":"2021-05-23T16:57:22.478385Z","iopub.execute_input":"2021-05-23T16:57:22.478993Z","iopub.status.idle":"2021-05-23T16:57:22.841412Z","shell.execute_reply.started":"2021-05-23T16:57:22.478954Z","shell.execute_reply":"2021-05-23T16:57:22.840458Z"},"trusted":true},"execution_count":137,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 200])\nB-per\tpresident\nI-per\tyushchenko\nO\tand\nO\ttymoshenko\nO\twere\nO\tonce\nO\tallies\nO\t,\nO\tbut\nO\ttheir\nO\trelationship\nO\tdeteriorated\nO\tbecause\nO\tof\nO\tbitter\nO\tpolitical\nO\tinfighting\nO\ta\nO\tyear\nO\tafter\nO\tthe\nO\t2004\nO\torange\nO\trevolution\nO\tthat\nO\tswept\nB-art\tmr\nO\t.\nB-art\tyushchenko\nB-art\tto\nB-art\tpower\nB-art\t.\n","output_type":"stream"}]}]}