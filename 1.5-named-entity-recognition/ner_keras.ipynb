{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import LSTM,Bidirectional,MaxPooling1D,Flatten,concatenate\n",
    "from tensorflow.keras.utils import Progbar\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tensorflow.keras.initializers import RandomUniform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadData():\n",
    "    def __init__(self):\n",
    "        self.train_files = None\n",
    "        self.validation_files = None\n",
    "        \n",
    "    def get_data(self):\n",
    "        self.train_files = glob.glob(\"benchmarking_data/Train//*.txt\")\n",
    "        self.validation_files = glob.glob(\"benchmarking_data/Validate//*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data_obj = LoadData()\n",
    "load_data_obj.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing():\n",
    "    def __init__(self):\n",
    "        self.word_embediings_model = open(\"embeddings/glove.6B.100d.txt\", encoding=\"utf-8\")\n",
    "         \n",
    "    \n",
    "    def sentence_from_file(self,filename):\n",
    "        f = open(filename)\n",
    "        single_file_sentences = []\n",
    "        sentence_list = []\n",
    "        for line in f:\n",
    "            if len(line)==0 or line[0]==\"\\n\":\n",
    "                if len(sentence_list) > 0:\n",
    "                    single_file_sentences.append(sentence_list)\n",
    "                    sentence_list = []\n",
    "                continue\n",
    "            splits = line.split(' ')\n",
    "            sentence_list.append([splits[0],splits[1],splits[-1]])\n",
    "\n",
    "        if len(sentence_list) >0:\n",
    "            single_file_sentences.append(sentence_list)\n",
    "            sentence_list = []\n",
    "        return single_file_sentences\n",
    "\n",
    "    def get_case_value(self,word, case_dict):   \n",
    "        case_value = 'other'\n",
    "\n",
    "        count_digits = 0\n",
    "        for char in word:\n",
    "            if char.isdigit():\n",
    "                count_digits += 1\n",
    "\n",
    "        if word.isdigit():\n",
    "            case_value = 'number'\n",
    "        elif count_digits / float(len(word)) > 0.5:\n",
    "            case_value = 'fraction'\n",
    "        elif word.islower():\n",
    "            case_value = 'lower'\n",
    "        elif word.isupper():\n",
    "            case_value = 'upper'\n",
    "        elif word[0].isupper():\n",
    "            case_value = 'title'\n",
    "        elif count_digits > 0:\n",
    "            case_value = 'leters_digit'\n",
    "\n",
    "        return case_dict[case_value]\n",
    "\n",
    "\n",
    "    def createBatches(self,data):\n",
    "        l = []\n",
    "        for i in data:\n",
    "            l.append(len(i[0]))\n",
    "        l = set(l)\n",
    "        batches = []\n",
    "        batch_len = []\n",
    "        z = 0\n",
    "        for i in l:\n",
    "            for batch in data:\n",
    "                if len(batch[0]) == i:\n",
    "                    batches.append(batch)\n",
    "                    z += 1\n",
    "            batch_len.append(z)\n",
    "        return batches,batch_len\n",
    "\n",
    "    def create_tensors(self,sentences,word_to_id,case_to_id,pos_to_id,char_to_id,label_to_id):\n",
    "        #paddingIdx = word2Idx['PAD_TKN']\n",
    "        unknownIdx = word_to_id['UNK_TKN']\n",
    "\n",
    "        dataset = []\n",
    "\n",
    "        word_count = 0\n",
    "        unknownword_count = 0\n",
    "\n",
    "        for sentence in sentences:\n",
    "            word_indices = []    \n",
    "            char_indices = []\n",
    "            case_indices = []\n",
    "            label_indices = []\n",
    "            pos_indices = []\n",
    "\n",
    "            for word,char,pos,label in sentence:  \n",
    "\n",
    "                word_count += 1\n",
    "                if word in word_to_id:\n",
    "                    word_index = word_to_id[word]\n",
    "                elif word.lower() in word_to_id:\n",
    "                    word_index = word_to_id[word.lower()]                 \n",
    "                else:\n",
    "                    word_index = unknownIdx\n",
    "                    unknownword_count += 1\n",
    "                    \n",
    "                char_index = []\n",
    "                for x in char:\n",
    "                    char_index.append(char_to_id[x])\n",
    "                    \n",
    "                word_indices.append(word_index)\n",
    "                case_indices.append(self.get_case_value(word, case_to_id))\n",
    "                pos_indices.append(pos_to_id[pos.replace('\\n','')])\n",
    "                char_indices.append(char_index)\n",
    "                label_indices.append(label_to_id[label])\n",
    "\n",
    "            dataset.append([word_indices, case_indices, char_indices, pos_indices, label_indices]) \n",
    "        return dataset\n",
    "\n",
    "\n",
    "    def addCharInformatioin(self,Sentences):\n",
    "        for i,sentence in enumerate(Sentences):\n",
    "            for j,data in enumerate(sentence):\n",
    "                chars = [c for c in data[0]]\n",
    "                Sentences[i][j] = [data[0],chars,data[1],data[2]]\n",
    "        return Sentences\n",
    "\n",
    "    def padding(self,Sentences):\n",
    "        maxlen = 52\n",
    "        for sentence in Sentences:\n",
    "            char = sentence[2]\n",
    "            for x in char:\n",
    "                maxlen = max(maxlen,len(x))\n",
    "        for i,sentence in enumerate(Sentences):\n",
    "            Sentences[i][2] = pad_sequences(Sentences[i][2],52,padding='post')\n",
    "        return Sentences\n",
    "    \n",
    "    def get_word_embeddings(self,list_sentences):\n",
    "        wd_to_id = {}\n",
    "        wd_em = []\n",
    "        \n",
    "        words = {}\n",
    "        for sentence in list_sentences:\n",
    "            for token,char,pos,label in sentence:\n",
    "                words[token.lower()] = True\n",
    "                \n",
    "        for line in self.word_embediings_model:\n",
    "            split = line.strip().split(\" \")\n",
    "\n",
    "            if len(wd_to_id) == 0:\n",
    "                wd_to_id[\"PAD_TKN\"] = len(wd_to_id)\n",
    "                vector = np.zeros(len(split)-1) \n",
    "                wd_em.append(vector)\n",
    "\n",
    "                wd_to_id[\"UNK_TKN\"] = len(wd_to_id)\n",
    "                vector = np.random.uniform(-0.25, 0.25, len(split)-1)\n",
    "                wd_em.append(vector)\n",
    "            if split[0].lower() in words:\n",
    "                vector = np.array([float(num) for num in split[1:]])\n",
    "                wd_em.append(vector)\n",
    "                wd_to_id[split[0]] = len(wd_to_id)\n",
    "\n",
    "        wd_em = np.array(wd_em)\n",
    "        return wd_em,wd_to_id\n",
    "    \n",
    "    def create_embeddings(self,list_sentences):\n",
    "\n",
    "        labelSet = set()\n",
    "        lb_to_id = {}\n",
    "        for sentence in list_sentences:\n",
    "            for token,char,pos,label in sentence:\n",
    "                labelSet.add(label)\n",
    "\n",
    "        for label in labelSet:\n",
    "            lb_to_id[label] = len(lb_to_id)\n",
    "\n",
    "        id_to_lb = {v: k for k, v in lb_to_id.items()}\n",
    "\n",
    "        ch_to_id = {\"PADDING\":0, \"UNKNOWN\":1}\n",
    "        for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|øæðş\":\n",
    "            ch_to_id[c] = len(ch_to_id)\n",
    "\n",
    "        cs_to_id = {\n",
    "                'number': 0, 'lower':1, 'upper':2, 'title':3, \n",
    "                'other':4, 'fraction':5, 'leters_digit': 6, \n",
    "                'PAD_TKN':7\n",
    "                }\n",
    "\n",
    "        pos_to_id = {\"$\":0, \"''\":1, \"(\":2, \")\":3, \",\":4, \"--\":5, \".\":6, \":\":7, \"CC\":8, \"CD\":9, \"DT\":10,\n",
    "                     \"EX\":11, \"FW\":12, \"IN\":13, \"JJ\":14, \"JJR\":15, \"JJS\":16, \"LS\":17, \"MD\":18, \"NN\":19,\n",
    "                     \"NNP\":20, \"NNPS\":21, \"NNS\":22, \"PDT\":23, \"POS\":24, \"PRP\":25, \"PRP$\":26, \"RB\":27, \n",
    "                     \"RBR\":28, \"RBS\":29, \"RP\":30, \"SYM\":31, \"TO\":32, \"UH\":33, \"VB\":34, \"VBD\":35, \"VBG\":36, \n",
    "                     \"VBN\":37, \"VBP\":38, \"VBZ\":39, \"WDT\":40, \"WP\":41, \"WP$\":42, \"WRB\":43, \"``\":44}\n",
    "        \n",
    "        return cs_to_id,pos_to_id,ch_to_id,lb_to_id,id_to_lb\n",
    "    \n",
    "    def make_batch(self,dataset):\n",
    "        self.batch,self.batch_len = self.createBatches(dataset)\n",
    "        return self.batch,self.batch_len\n",
    "        \n",
    "    def make_dataset(self,file_name):\n",
    "        sentences = self.sentence_from_file(file_name)\n",
    "        sentences = self.addCharInformatioin(sentences)\n",
    "        return sentences\n",
    "    \n",
    "    def get_sentences(self,file_list):\n",
    "        list_sentences = []\n",
    "        for i in file_list:\n",
    "            list_sentences+= self.make_dataset(i)\n",
    "        return list_sentences\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_obj = Preprocessing()\n",
    "train_sentences = preprocess_obj.get_sentences(load_data_obj.train_files)\n",
    "word_emb,word_to_id = preprocess_obj.get_word_embeddings(train_sentences)\n",
    "\n",
    "'''the below function is not requred for validation data, we will load the dictionaries for validation'''\n",
    "case_to_id,pos_to_id,char_to_id,label_to_id,id_to_label = preprocess_obj.create_embeddings(train_sentences)\n",
    "train_data_set = preprocess_obj.padding(preprocess_obj.create_tensors(train_sentences,word_to_id,case_to_id,pos_to_id,char_to_id,label_to_id))\n",
    "train_batch,train_batch_len = preprocess_obj.make_batch(train_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DesignModel():\n",
    "    def __init__(self,params):\n",
    "        self.model = None\n",
    "        self.wd_em = word_emb\n",
    "        self.caseEmbeddings = np.identity(len(case_to_id), dtype='float32')\n",
    "        self.posEmbeddings = np.identity(len(pos_to_id), dtype='float32') \n",
    "        self.ch_to_id = char_to_id\n",
    "        self.lb_to_id = label_to_id\n",
    "        self.params = params\n",
    "        self.train_batch = train_batch\n",
    "        self.train_batch_len = train_batch_len\n",
    "\n",
    "        \n",
    "    def iterate_minibatches(self,dataset,batch_len): \n",
    "        start = 0\n",
    "        for i in batch_len:\n",
    "            tokens = []\n",
    "            char = []\n",
    "            labels = []\n",
    "            casing = []\n",
    "            pos_tags = []\n",
    "            data = dataset[start:i]\n",
    "            start = i\n",
    "            for dt in data:\n",
    "                t,c,ch,pos,l = dt\n",
    "                l = np.expand_dims(l,-1)\n",
    "                tokens.append(t)\n",
    "                char.append(ch)\n",
    "                labels.append(l)\n",
    "                casing.append(c)\n",
    "                pos_tags.append(pos)\n",
    "            yield np.asarray(labels),np.asarray(tokens),np.asarray(casing), np.asarray(char), np.asarray(pos_tags)\n",
    "    \n",
    "    def BiRNN_model(self):\n",
    "    \n",
    "        input = Input(shape=(None,),dtype='int32')\n",
    "\n",
    "        words = Embedding(input_dim=self.wd_em.shape[0], output_dim=self.wd_em.shape[1],  weights=[self.wd_em], trainable=False)(input)\n",
    "\n",
    "        csng_input = Input(shape=(None,), dtype='int32')\n",
    "        csng = Embedding(output_dim = self.caseEmbeddings.shape[1], input_dim = self.caseEmbeddings.shape[0], weights = [self.caseEmbeddings], trainable=False)(csng_input)\n",
    "\n",
    "\n",
    "        char_input=Input(shape=(None,52,))\n",
    "        embed_char_out=TimeDistributed(Embedding(len(self.ch_to_id),30,embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)))(char_input)\n",
    "        dropout= Dropout(self.params['dropout_rate'])(embed_char_out)\n",
    "        conv1d_out = TimeDistributed(Conv1D(kernel_size=self.params['kernel_sizes_cnn'], filters=30, padding='same',activation=params['rnn_activation'], strides=1))(dropout)\n",
    "        maxpool_out=TimeDistributed(MaxPooling1D(52))(conv1d_out)\n",
    "        char = TimeDistributed(Flatten())(maxpool_out)\n",
    "        char = Dropout(self.params['dropout_rate'])(char)\n",
    "\n",
    "        pos_input = Input(shape=(None,), dtype='int32')\n",
    "        pos = Embedding(output_dim = self.posEmbeddings.shape[1], input_dim = self.posEmbeddings.shape[0], weights = [self.posEmbeddings], trainable=False)(pos_input)\n",
    "\n",
    "\n",
    "        output = concatenate([words, csng, char, pos])\n",
    "        output = Bidirectional(LSTM(self.params['units_lstm'], return_sequences=True, dropout=self.params['dropout_rate'], recurrent_dropout=0.25))(output)\n",
    "        output = TimeDistributed(Dense(len(self.lb_to_id), activation=self.params['rnn_activation']))(output)\n",
    "        self.model = Model(inputs=[input, csng_input, char_input, pos_input], outputs=[output])\n",
    "        self.model.compile(loss=self.params['loss'], optimizer=self.params['optimizer'],metrics=[\"accuracy\"])\n",
    "\n",
    "    def train_model(self):\n",
    "    \n",
    "        for epoch in range(self.params['epochs']):\n",
    "\n",
    "            print(\"Epoch %d/%d\"%(epoch+1, self.params['epochs']))\n",
    "            a = Progbar(len(preprocess_obj.batch_len))\n",
    "            res = None\n",
    "            for i,batch in enumerate(self.iterate_minibatches(self.train_batch,self.train_batch_len)):\n",
    "                labels, tkns, csng, char, pos = batch       \n",
    "                res = self.model.train_on_batch([tkns, csng, char, pos], labels)\n",
    "                a.update(i)\n",
    "            print(\"\\n\")\n",
    "            print(self.model.metrics_names[0],\":\",res[0],self.model.metrics_names[1],\":\",res[1])\n",
    "            print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 2.3684568 acc : 0.4857143\n",
      " \n",
      "Epoch 2/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 2.0731783 acc : 0.4857143\n",
      " \n",
      "Epoch 3/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 1.9520473 acc : 0.4857143\n",
      " \n",
      "Epoch 4/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 1.8190781 acc : 0.51428574\n",
      " \n",
      "Epoch 5/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 1.5108054 acc : 0.6\n",
      " \n",
      "Epoch 6/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 1.1652856 acc : 0.6857143\n",
      " \n",
      "Epoch 7/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 1.0051807 acc : 0.6571429\n",
      " \n",
      "Epoch 8/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.96359015 acc : 0.74285716\n",
      " \n",
      "Epoch 9/100\n",
      "27/29 [==========================>...] - ETA: 0s\n",
      "\n",
      "loss : 0.9899187 acc : 0.71428573\n",
      " \n",
      "Epoch 10/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.7701035 acc : 0.74285716\n",
      " \n",
      "Epoch 11/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.6495928 acc : 0.7714286\n",
      " \n",
      "Epoch 12/100\n",
      "27/29 [==========================>...] - ETA: 0s\n",
      "\n",
      "loss : 0.6091892 acc : 0.74285716\n",
      " \n",
      "Epoch 13/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.44169152 acc : 0.82857144\n",
      " \n",
      "Epoch 14/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.58594006 acc : 0.8\n",
      " \n",
      "Epoch 15/100\n",
      "27/29 [==========================>...] - ETA: 0s\n",
      "\n",
      "loss : 0.44309366 acc : 0.8\n",
      " \n",
      "Epoch 16/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.4333148 acc : 0.85714287\n",
      " \n",
      "Epoch 17/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.30044508 acc : 0.9142857\n",
      " \n",
      "Epoch 18/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.33469483 acc : 0.94285715\n",
      " \n",
      "Epoch 19/100\n",
      "27/29 [==========================>...] - ETA: 0s\n",
      "\n",
      "loss : 0.5239522 acc : 0.85714287\n",
      " \n",
      "Epoch 20/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.33912036 acc : 0.8857143\n",
      " \n",
      "Epoch 21/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.2053036 acc : 0.9714286\n",
      " \n",
      "Epoch 22/100\n",
      "27/29 [==========================>...] - ETA: 0s\n",
      "\n",
      "loss : 0.41610777 acc : 0.82857144\n",
      " \n",
      "Epoch 23/100\n",
      "27/29 [==========================>...] - ETA: 0s\n",
      "\n",
      "loss : 0.27040768 acc : 0.94285715\n",
      " \n",
      "Epoch 24/100\n",
      "27/29 [==========================>...] - ETA: 0s\n",
      "\n",
      "loss : 0.17774571 acc : 0.9714286\n",
      " \n",
      "Epoch 25/100\n",
      "27/29 [==========================>...] - ETA: 0s\n",
      "\n",
      "loss : 0.14066264 acc : 0.9714286\n",
      " \n",
      "Epoch 26/100\n",
      "27/29 [==========================>...] - ETA: 0s\n",
      "\n",
      "loss : 0.18579224 acc : 0.9142857\n",
      " \n",
      "Epoch 27/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.15730356 acc : 0.9714286\n",
      " \n",
      "Epoch 28/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.1870333 acc : 1.0\n",
      " \n",
      "Epoch 29/100\n",
      "27/29 [==========================>...] - ETA: 0s\n",
      "\n",
      "loss : 0.16420953 acc : 1.0\n",
      " \n",
      "Epoch 30/100\n",
      "27/29 [==========================>...] - ETA: 0s\n",
      "\n",
      "loss : 0.124701865 acc : 1.0\n",
      " \n",
      "Epoch 31/100\n",
      "27/29 [==========================>...] - ETA: 0s\n",
      "\n",
      "loss : 0.15248969 acc : 0.9714286\n",
      " \n",
      "Epoch 32/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.22287686 acc : 0.9142857\n",
      " \n",
      "Epoch 33/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.11004705 acc : 0.9714286\n",
      " \n",
      "Epoch 34/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.09510437 acc : 1.0\n",
      " \n",
      "Epoch 35/100\n",
      "27/29 [==========================>...] - ETA: 0s\n",
      "\n",
      "loss : 0.088632 acc : 1.0\n",
      " \n",
      "Epoch 36/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.13185365 acc : 0.94285715\n",
      " \n",
      "Epoch 37/100\n",
      "27/29 [==========================>...] - ETA: 0s\n",
      "\n",
      "loss : 0.10320226 acc : 1.0\n",
      " \n",
      "Epoch 38/100\n",
      "27/29 [==========================>...] - ETA: 0s\n",
      "\n",
      "loss : 0.097845666 acc : 1.0\n",
      " \n",
      "Epoch 39/100\n",
      "27/29 [==========================>...] - ETA: 0s\n",
      "\n",
      "loss : 0.072456576 acc : 0.9714286\n",
      " \n",
      "Epoch 40/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.09357626 acc : 0.9714286\n",
      " \n",
      "Epoch 41/100\n",
      "27/29 [==========================>...] - ETA: 0s\n",
      "\n",
      "loss : 0.12393751 acc : 1.0\n",
      " \n",
      "Epoch 42/100\n",
      "27/29 [==========================>...] - ETA: 0s\n",
      "\n",
      "loss : 0.060085118 acc : 1.0\n",
      " \n",
      "Epoch 43/100\n",
      "27/29 [==========================>...] - ETA: 0s\n",
      "\n",
      "loss : 0.09942885 acc : 1.0\n",
      " \n",
      "Epoch 44/100\n",
      "27/29 [==========================>...] - ETA: 0s\n",
      "\n",
      "loss : 0.06280138 acc : 1.0\n",
      " \n",
      "Epoch 45/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.07149848 acc : 1.0\n",
      " \n",
      "Epoch 46/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.045161784 acc : 1.0\n",
      " \n",
      "Epoch 47/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.08160774 acc : 0.9714286\n",
      " \n",
      "Epoch 48/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.035861827 acc : 1.0\n",
      " \n",
      "Epoch 49/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.08538043 acc : 0.9714286\n",
      " \n",
      "Epoch 50/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.07122853 acc : 1.0\n",
      " \n",
      "Epoch 51/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.040840533 acc : 1.0\n",
      " \n",
      "Epoch 52/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.05800392 acc : 1.0\n",
      " \n",
      "Epoch 53/100\n",
      "27/29 [==========================>...] - ETA: 1s\n",
      "\n",
      "loss : 0.0800502 acc : 1.0\n",
      " \n",
      "Epoch 54/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.038355656 acc : 1.0\n",
      " \n",
      "Epoch 55/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.064246945 acc : 1.0\n",
      " \n",
      "Epoch 56/100\n",
      "27/29 [==========================>...] - ETA: 0s\n",
      "\n",
      "loss : 0.08732472 acc : 0.9714286\n",
      " \n",
      "Epoch 57/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.128126 acc : 0.9714286\n",
      " \n",
      "Epoch 58/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.025848081 acc : 1.0\n",
      " \n",
      "Epoch 59/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.030282568 acc : 1.0\n",
      " \n",
      "Epoch 60/100\n",
      "27/29 [==========================>...] - ETA: 0s\n",
      "\n",
      "loss : 0.071419746 acc : 0.9714286\n",
      " \n",
      "Epoch 61/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.04583656 acc : 1.0\n",
      " \n",
      "Epoch 62/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.022173543 acc : 1.0\n",
      " \n",
      "Epoch 63/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.10266268 acc : 0.9714286\n",
      " \n",
      "Epoch 64/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.10363147 acc : 0.9714286\n",
      " \n",
      "Epoch 65/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.03489538 acc : 1.0\n",
      " \n",
      "Epoch 66/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.022138834 acc : 1.0\n",
      " \n",
      "Epoch 67/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.031859655 acc : 1.0\n",
      " \n",
      "Epoch 68/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.046790175 acc : 1.0\n",
      " \n",
      "Epoch 69/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.026803246 acc : 1.0\n",
      " \n",
      "Epoch 70/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.014136387 acc : 1.0\n",
      " \n",
      "Epoch 71/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.03431965 acc : 1.0\n",
      " \n",
      "Epoch 72/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.015458025 acc : 1.0\n",
      " \n",
      "Epoch 73/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.015178506 acc : 1.0\n",
      " \n",
      "Epoch 74/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.024208419 acc : 1.0\n",
      " \n",
      "Epoch 75/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.012940623 acc : 1.0\n",
      " \n",
      "Epoch 76/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.012868389 acc : 1.0\n",
      " \n",
      "Epoch 77/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.059746645 acc : 0.9714286\n",
      " \n",
      "Epoch 78/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.03965766 acc : 1.0\n",
      " \n",
      "Epoch 79/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.016803294 acc : 1.0\n",
      " \n",
      "Epoch 80/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.020727763 acc : 1.0\n",
      " \n",
      "Epoch 81/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.020674922 acc : 1.0\n",
      " \n",
      "Epoch 82/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.02040442 acc : 1.0\n",
      " \n",
      "Epoch 83/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.015770214 acc : 1.0\n",
      " \n",
      "Epoch 84/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.02060849 acc : 1.0\n",
      " \n",
      "Epoch 85/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.041184884 acc : 0.9714286\n",
      " \n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.016449759 acc : 1.0\n",
      " \n",
      "Epoch 87/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.038723964 acc : 1.0\n",
      " \n",
      "Epoch 88/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.016473642 acc : 1.0\n",
      " \n",
      "Epoch 89/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.01188314 acc : 1.0\n",
      " \n",
      "Epoch 90/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.012220089 acc : 1.0\n",
      " \n",
      "Epoch 91/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.007815264 acc : 1.0\n",
      " \n",
      "Epoch 92/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.0073975376 acc : 1.0\n",
      " \n",
      "Epoch 93/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.0067232563 acc : 1.0\n",
      " \n",
      "Epoch 94/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.013154262 acc : 1.0\n",
      " \n",
      "Epoch 95/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.012299427 acc : 1.0\n",
      " \n",
      "Epoch 96/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.0110901855 acc : 1.0\n",
      " \n",
      "Epoch 97/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.0082137855 acc : 1.0\n",
      " \n",
      "Epoch 98/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.027175779 acc : 1.0\n",
      " \n",
      "Epoch 99/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.014137611 acc : 1.0\n",
      " \n",
      "Epoch 100/100\n",
      "28/29 [===========================>..] - ETA: 0s\n",
      "\n",
      "loss : 0.008850768 acc : 1.0\n",
      " \n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "            \"kernel_sizes_cnn\": 3,\n",
    "            \"optimizer\": \"nadam\",\n",
    "            \"cnn_activation\":\"tanh\",\n",
    "            \"rnn_activation\":\"softmax\",\n",
    "            \"units_lstm\" : 100,\n",
    "            \"loss\": \"sparse_categorical_crossentropy\",\n",
    "            \"text_size\": 50,\n",
    "            \"dropout_rate\": 0.5,\n",
    "            \"epochs\": 100,\n",
    "            \"model_name\": \"cnn_model\",\n",
    "            \"batch_size\": 32,\n",
    "            \"verbose\": True,\n",
    "            \"metrics\":[\"accuracy\"]\n",
    "        }\n",
    "model_obj = DesignModel(params)\n",
    "model_obj.BiRNN_model()\n",
    "model_obj.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadAndSaveModels():\n",
    "        \n",
    "    def save_model(self,model,model_name):\n",
    "        model.save(\"Model_Data/entity_models/\"+model_name+\".h5\")\n",
    "        print(\"Model saved to Model folder.\")\n",
    "        \n",
    "    def save_dict(self, save_path,dictionaries):  \n",
    "        \n",
    "        for item in dictionaries:\n",
    "            \n",
    "            with open(save_path+\"/\"+item[1]+\".txt\", \"wb\") as myFile:\n",
    "                pickle.dump(item[0], myFile)\n",
    "\n",
    "        print(\"Files saved.\")\n",
    "        \n",
    "    def load_dict(self,file):\n",
    "        with open(file,\"rb\") as fp:\n",
    "            dict = pickle.load(fp)\n",
    "        return dict\n",
    "    \n",
    "    def load_model(self,model_name):\n",
    "        model = load_model(model_name)\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to Model folder.\n",
      "Files saved.\n"
     ]
    }
   ],
   "source": [
    "load_save = LoadAndSaveModels()\n",
    "load_save.save_model(model_obj.model,\"birnn\")\n",
    "dict = [(word_to_id,\"word_to_id\"),(label_to_id,\"label_to_id\"),(char_to_id,\"char_to_id\"),\n",
    "        (id_to_label,\"id_to_label\"),(case_to_id,\"case_to_id\"),(pos_to_id,\"pos_to_id\")]\n",
    "load_save.save_dict(\"Model_Data/dict\",dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_save = LoadAndSaveModels()\n",
    "model = load_save.load_model(\"Model_Data/entity_models/birnn.h5\")\n",
    "word_to_id = load_save.load_dict(\"Model_Data/dict/word_to_id.txt\")\n",
    "case_to_id = load_save.load_dict(\"Model_Data/dict/case_to_id.txt\")\n",
    "pos_to_id = load_save.load_dict(\"Model_Data/dict/pos_to_id.txt\")\n",
    "char_to_id = load_save.load_dict(\"Model_Data/dict/char_to_id.txt\")\n",
    "label_to_id = load_save.load_dict(\"Model_Data/dict/label_to_id.txt\")\n",
    "id_to_label = load_save.load_dict(\"Model_Data/dict/id_to_label.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_sentences = preprocess_obj.get_sentences(load_data_obj.validation_files)\n",
    "validation_set = preprocess_obj.padding(preprocess_obj.create_tensors(validation_sentences,word_to_id,case_to_id,pos_to_id,char_to_id,label_to_id))\n",
    "validation_batch,validation_batch_len = preprocess_obj.make_batch(validation_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction():\n",
    "    def __init__(self):\n",
    "        self.case_to_id = case_to_id\n",
    "        self.pos_to_id = pos_to_id\n",
    "        self.char_to_id = char_to_id\n",
    "        self.label_to_id = label_to_id\n",
    "        self.id_to_label = id_to_label\n",
    "        self.word_to_id = word_to_id\n",
    "    def prediction(self,dataset,model):\n",
    "        correct_labels = []\n",
    "        predict_labels = []\n",
    "        b = Progbar(len(dataset))\n",
    "        for i,data in enumerate(dataset):    \n",
    "            tkns, csng, char,pos, labels = data\n",
    "            tkns = np.asarray([tkns])     \n",
    "            char = np.asarray([char])\n",
    "            csng = np.asarray([csng])\n",
    "            pos = np.asarray([pos])\n",
    "            predict = model.predict([tkns, csng, char,pos], verbose=False)[0] \n",
    "            predict = predict.argmax(axis=-1)        \n",
    "            correct_labels.append(labels)\n",
    "            predict_labels.append(predict)\n",
    "            b.update(i)\n",
    "        return predict_labels, correct_labels\n",
    "    \n",
    "    def predict(self,sentence,model):\n",
    "        sen_list = [[[i,'POS','O\\n'] for i in sentence.split()]]\n",
    "        test_sent = preprocess_obj.addCharInformatioin(sen_list)\n",
    "\n",
    "        predLabels = []\n",
    "\n",
    "        test_set = preprocess_obj.padding(preprocess_obj.create_tensors(test_sent,self.word_to_id,\n",
    "                                                                        self.case_to_id,self.pos_to_id,\n",
    "                                                                        self.char_to_id,self.label_to_id))\n",
    "        test_batch,test_batch_len = preprocess_obj.createBatches(test_set)\n",
    "        for i,data in enumerate(test_batch):\n",
    "            tokens, csng, char, pos, labels = data\n",
    "            tokens = np.asarray([tokens])     \n",
    "            char = np.asarray([char])\n",
    "            csng = np.asarray([csng])\n",
    "            pos = np.asarray([pos])\n",
    "            pred = model.predict([tokens,csng, char,pos], verbose=False)[0] \n",
    "            pred = pred.argmax(axis=-1) #Predict the classes            \n",
    "            predLabels.append(pred)\n",
    "        entity_labels = []\n",
    "        j = 0\n",
    "        words_list = sentence.split()\n",
    "        for i in predLabels[-1]:\n",
    "            entity_labels.append((words_list[j],self.id_to_label[int(i)].replace(\"\\n\",\"\")))\n",
    "            j+=1\n",
    "\n",
    "        return entity_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_obj = Prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sent = \"Add Richard McNamara newest song to the Just Smile playlist\"\n",
    "entity_label = pred_obj.predict(sent,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Add', 'O'),\n",
       " ('Richard', 'B-artist'),\n",
       " ('McNamara', 'I-artist'),\n",
       " ('newest', 'O'),\n",
       " ('song', 'B-music_item'),\n",
       " ('to', 'O'),\n",
       " ('the', 'O'),\n",
       " ('Just', 'B-playlist'),\n",
       " ('Smile', 'I-playlist'),\n",
       " ('playlist', 'O')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluate():\n",
    "    def compute_precision(self,guessed_sentences, correct_sentences):\n",
    "        assert(len(guessed_sentences) == len(correct_sentences))\n",
    "        correctCount = 0\n",
    "        count = 0\n",
    "\n",
    "\n",
    "        for sentenceIdx in range(len(guessed_sentences)):\n",
    "            guessed = guessed_sentences[sentenceIdx]\n",
    "            correct = correct_sentences[sentenceIdx]\n",
    "            assert(len(guessed) == len(correct))\n",
    "            idx = 0\n",
    "            while idx < len(guessed):\n",
    "                if guessed[idx][0] == 'B': #A new chunk starts\n",
    "                    count += 1\n",
    "\n",
    "                    if guessed[idx] == correct[idx]:\n",
    "                        idx += 1\n",
    "                        correctlyFound = True\n",
    "\n",
    "                        while idx < len(guessed) and guessed[idx][0] == 'I': #Scan until it no longer starts with I\n",
    "                            if guessed[idx] != correct[idx]:\n",
    "                                correctlyFound = False\n",
    "\n",
    "                            idx += 1\n",
    "\n",
    "                        if idx < len(guessed):\n",
    "                            if correct[idx][0] == 'I': #The chunk in correct was longer\n",
    "                                correctlyFound = False\n",
    "\n",
    "\n",
    "                        if correctlyFound:\n",
    "                            correctCount += 1\n",
    "                    else:\n",
    "                        idx += 1\n",
    "                else:  \n",
    "                    idx += 1\n",
    "\n",
    "        precision = 0\n",
    "        if count > 0:    \n",
    "            precision = float(correctCount) / count\n",
    "\n",
    "        return precision\n",
    "    def get_metrics(self,predictions, correct, idx2Label): \n",
    "        label_pred = []    \n",
    "        for sentence in predictions:\n",
    "            label_pred.append([idx2Label[element] for element in sentence])\n",
    "\n",
    "        label_correct = []    \n",
    "        for sentence in correct:\n",
    "            label_correct.append([idx2Label[element] for element in sentence])\n",
    "\n",
    "\n",
    "        #print label_pred\n",
    "        #print label_correct\n",
    "\n",
    "        prec = self.compute_precision(label_pred, label_correct)\n",
    "        rec = self.compute_precision(label_correct, label_pred)\n",
    "\n",
    "        f1 = 0\n",
    "        if (rec+prec) > 0:\n",
    "            f1 = 2.0 * prec * rec / (prec + rec);\n",
    "\n",
    "        return prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13780/13784 [============================>.] - ETA: 0sTrain-Data: Precision: 0.924, Recall: 0.929, F1 Score: 0.927\n",
      "694/700 [============================>.] - ETA: 0sValidation-Data: Precision: 0.904, Recall: 0.905, F1 Score: 0.905\n"
     ]
    }
   ],
   "source": [
    "eval_obj = Evaluate()\n",
    "\n",
    "train_predict_labels, train_correct_labels = pred_obj.prediction(train_data_set,model)\n",
    "pre_train, rec_train, f1_train= eval_obj.get_metrics(train_predict_labels, train_correct_labels, id_to_label)\n",
    "print(\"Train-Data: Precision: %.3f, Recall: %.3f, F1 Score: %.3f\" % (pre_train, rec_train, f1_train))\n",
    "     \n",
    "validation_predict_labels, validation_correct_labels = pred_obj.prediction(validation_set,model)\n",
    "pre_test, rec_test, f1_test= eval_obj.get_metrics(validation_predict_labels, validation_correct_labels, id_to_label)\n",
    "print(\"Validation-Data: Precision: %.3f, Recall: %.3f, F1 Score: %.3f\" % (pre_test, rec_test, f1_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
