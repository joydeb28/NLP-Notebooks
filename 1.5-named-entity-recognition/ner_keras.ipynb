{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data i used  https://github.com/snipsco/nlu-benchmark/tree/master/2017-06-custom-intent-engines\n",
    "Then you can run data_making.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import LSTM,Bidirectional,MaxPooling1D,Flatten,concatenate\n",
    "from tensorflow.keras.utils import Progbar\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tensorflow.keras.initializers import RandomUniform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadData():\n",
    "    def __init__(self):\n",
    "        self.train_files = None\n",
    "        self.validation_files = None\n",
    "        \n",
    "    def get_data(self):\n",
    "        self.train_files = glob.glob(\"benchmarking_data/Train//*.txt\")\n",
    "        self.validation_files = glob.glob(\"benchmarking_data/Validate//*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data_obj = LoadData()\n",
    "load_data_obj.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing():\n",
    "    def __init__(self):\n",
    "        self.word_embediings_model = open(\"embeddings/glove.6B.100d.txt\", encoding=\"utf-8\")\n",
    "         \n",
    "    \n",
    "    def sentence_from_file(self,filename):\n",
    "        f = open(filename)\n",
    "        single_file_sentences = []\n",
    "        sentence_list = []\n",
    "        for line in f:\n",
    "            if len(line)==0 or line[0]==\"\\n\":\n",
    "                if len(sentence_list) > 0:\n",
    "                    single_file_sentences.append(sentence_list)\n",
    "                    sentence_list = []\n",
    "                continue\n",
    "            splits = line.split(' ')\n",
    "            sentence_list.append([splits[0],splits[1],splits[-1]])\n",
    "\n",
    "        if len(sentence_list) >0:\n",
    "            single_file_sentences.append(sentence_list)\n",
    "            sentence_list = []\n",
    "        return single_file_sentences\n",
    "\n",
    "    def get_case_value(self,word, case_dict):   \n",
    "        case_value = 'other'\n",
    "\n",
    "        count_digits = 0\n",
    "        for char in word:\n",
    "            if char.isdigit():\n",
    "                count_digits += 1\n",
    "\n",
    "        if word.isdigit():\n",
    "            case_value = 'number'\n",
    "        elif count_digits / float(len(word)) > 0.5:\n",
    "            case_value = 'fraction'\n",
    "        elif word.islower():\n",
    "            case_value = 'lower'\n",
    "        elif word.isupper():\n",
    "            case_value = 'upper'\n",
    "        elif word[0].isupper():\n",
    "            case_value = 'title'\n",
    "        elif count_digits > 0:\n",
    "            case_value = 'leters_digit'\n",
    "\n",
    "        return case_dict[case_value]\n",
    "\n",
    "\n",
    "    def createBatches(self,data):\n",
    "        l = []\n",
    "        for i in data:\n",
    "            l.append(len(i[0]))\n",
    "        l = set(l)\n",
    "        batches = []\n",
    "        batch_len = []\n",
    "        z = 0\n",
    "        for i in l:\n",
    "            for batch in data:\n",
    "                if len(batch[0]) == i:\n",
    "                    batches.append(batch)\n",
    "                    z += 1\n",
    "            batch_len.append(z)\n",
    "        return batches,batch_len\n",
    "\n",
    "    def create_tensors(self,sentences,word_to_id,case_to_id,pos_to_id,char_to_id,label_to_id):\n",
    "        #paddingIdx = word2Idx['PAD_TKN']\n",
    "        unknownIdx = word_to_id['UNK_TKN']\n",
    "\n",
    "        dataset = []\n",
    "\n",
    "        word_count = 0\n",
    "        unknownword_count = 0\n",
    "\n",
    "        for sentence in sentences:\n",
    "            word_indices = []    \n",
    "            char_indices = []\n",
    "            case_indices = []\n",
    "            label_indices = []\n",
    "            pos_indices = []\n",
    "\n",
    "            for word,char,pos,label in sentence:  \n",
    "\n",
    "                word_count += 1\n",
    "                if word in word_to_id:\n",
    "                    word_index = word_to_id[word]\n",
    "                elif word.lower() in word_to_id:\n",
    "                    word_index = word_to_id[word.lower()]                 \n",
    "                else:\n",
    "                    word_index = unknownIdx\n",
    "                    unknownword_count += 1\n",
    "                    \n",
    "                char_index = []\n",
    "                for x in char:\n",
    "                    char_index.append(char_to_id[x])\n",
    "                    \n",
    "                word_indices.append(word_index)\n",
    "                case_indices.append(self.get_case_value(word, case_to_id))\n",
    "                pos_indices.append(pos_to_id[pos.replace('\\n','')])\n",
    "                char_indices.append(char_index)\n",
    "                label_indices.append(label_to_id[label])\n",
    "            print([word_indices, case_indices, char_indices, pos_indices, label_indices])\n",
    "            dataset.append([word_indices, case_indices, char_indices, pos_indices, label_indices]) \n",
    "        return dataset\n",
    "\n",
    "\n",
    "    def addCharInformatioin(self,Sentences):\n",
    "        for i,sentence in enumerate(Sentences):\n",
    "            for j,data in enumerate(sentence):\n",
    "                chars = [c for c in data[0]]\n",
    "                Sentences[i][j] = [data[0],chars,data[1],data[2]]\n",
    "        return Sentences\n",
    "\n",
    "    def padding(self,Sentences):\n",
    "        maxlen = 52\n",
    "        for sentence in Sentences:\n",
    "            char = sentence[2]\n",
    "            for x in char:\n",
    "                maxlen = max(maxlen,len(x))\n",
    "        for i,sentence in enumerate(Sentences):\n",
    "            Sentences[i][2] = pad_sequences(Sentences[i][2],52,padding='post')\n",
    "        return Sentences\n",
    "    \n",
    "    def get_word_embeddings(self,list_sentences):\n",
    "        wd_to_id = {}\n",
    "        wd_em = []\n",
    "        \n",
    "        words = {}\n",
    "        for sentence in list_sentences:\n",
    "            for token,char,pos,label in sentence:\n",
    "                words[token.lower()] = True\n",
    "                \n",
    "        for line in self.word_embediings_model:\n",
    "            split = line.strip().split(\" \")\n",
    "\n",
    "            if len(wd_to_id) == 0:\n",
    "                wd_to_id[\"PAD_TKN\"] = len(wd_to_id)\n",
    "                vector = np.zeros(len(split)-1) \n",
    "                wd_em.append(vector)\n",
    "\n",
    "                wd_to_id[\"UNK_TKN\"] = len(wd_to_id)\n",
    "                vector = np.random.uniform(-0.25, 0.25, len(split)-1)\n",
    "                wd_em.append(vector)\n",
    "            if split[0].lower() in words:\n",
    "                vector = np.array([float(num) for num in split[1:]])\n",
    "                wd_em.append(vector)\n",
    "                wd_to_id[split[0]] = len(wd_to_id)\n",
    "\n",
    "        wd_em = np.array(wd_em)\n",
    "        return wd_em,wd_to_id\n",
    "    \n",
    "    def get_feature_dict(self,sentences):\n",
    "\n",
    "        labelSet = set()\n",
    "        lb_to_id = {}\n",
    "        for sentence in sentences:\n",
    "            for token,char,pos,label in sentence:\n",
    "                labelSet.add(label)\n",
    "\n",
    "        for label in labelSet:\n",
    "            lb_to_id[label] = len(lb_to_id)\n",
    "\n",
    "        id_to_lb = {v: k for k, v in lb_to_id.items()}\n",
    "\n",
    "        ch_to_id = {\"PADDING\":0, \"UNKNOWN\":1}\n",
    "        for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|øæðş\":\n",
    "            ch_to_id[c] = len(ch_to_id)\n",
    "\n",
    "        cs_to_id = {\n",
    "                'number': 0, 'lower':1, 'upper':2, 'title':3, \n",
    "                'other':4, 'fraction':5, 'leters_digit': 6, \n",
    "                'PAD_TKN':7\n",
    "                }\n",
    "\n",
    "        pos_to_id = {\"$\":0, \"''\":1, \"(\":2, \")\":3, \",\":4, \"--\":5, \".\":6, \":\":7, \"CC\":8, \"CD\":9, \"DT\":10,\n",
    "                     \"EX\":11, \"FW\":12, \"IN\":13, \"JJ\":14, \"JJR\":15, \"JJS\":16, \"LS\":17, \"MD\":18, \"NN\":19,\n",
    "                     \"NNP\":20, \"NNPS\":21, \"NNS\":22, \"PDT\":23, \"POS\":24, \"PRP\":25, \"PRP$\":26, \"RB\":27, \n",
    "                     \"RBR\":28, \"RBS\":29, \"RP\":30, \"SYM\":31, \"TO\":32, \"UH\":33, \"VB\":34, \"VBD\":35, \"VBG\":36, \n",
    "                     \"VBN\":37, \"VBP\":38, \"VBZ\":39, \"WDT\":40, \"WP\":41, \"WP$\":42, \"WRB\":43, \"``\":44}\n",
    "        \n",
    "        return cs_to_id,pos_to_id,ch_to_id,lb_to_id,id_to_lb\n",
    "    \n",
    "    def make_batch(self,dataset):\n",
    "        self.batch,self.batch_len = self.createBatches(dataset)\n",
    "        return self.batch,self.batch_len\n",
    "        \n",
    "    def make_dataset(self,file_name):\n",
    "        sentences = self.sentence_from_file(file_name)\n",
    "        sentences = self.addCharInformatioin(sentences)\n",
    "        return sentences\n",
    "    \n",
    "    def get_sentences(self,file_list):\n",
    "        list_sentences = []\n",
    "        for i in file_list:\n",
    "            list_sentences+= self.make_dataset(i)\n",
    "        return list_sentences\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_obj = Preprocessing()\n",
    "train_sentences = preprocess_obj.get_sentences(load_data_obj.train_files)\n",
    "word_emb,word_to_id = preprocess_obj.get_word_embeddings(train_sentences)\n",
    "\n",
    "'''the below function is not requred for validation data, we will load the dictionaries for validation'''\n",
    "case_to_id,pos_to_id,char_to_id,label_to_id,id_to_label = preprocess_obj.get_feature_dict(train_sentences)\n",
    "train_data_set = preprocess_obj.padding(preprocess_obj.create_tensors(train_sentences,word_to_id,case_to_id,pos_to_id,char_to_id,label_to_id))\n",
    "train_batch,train_batch_len = preprocess_obj.make_batch(train_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DesignModel():\n",
    "    def __init__(self,params):\n",
    "        self.model = None\n",
    "        self.wd_em = word_emb\n",
    "        self.caseEmbeddings = np.identity(len(case_to_id), dtype='float32')\n",
    "        self.posEmbeddings = np.identity(len(pos_to_id), dtype='float32') \n",
    "        self.ch_to_id = char_to_id\n",
    "        self.lb_to_id = label_to_id\n",
    "        self.params = params\n",
    "        self.train_batch = train_batch\n",
    "        self.train_batch_len = train_batch_len\n",
    "\n",
    "        \n",
    "    def iterate_minibatches(self,dataset,batch_len): \n",
    "        start = 0\n",
    "        for i in batch_len:\n",
    "            tokens = []\n",
    "            char = []\n",
    "            labels = []\n",
    "            casing = []\n",
    "            pos_tags = []\n",
    "            data = dataset[start:i]\n",
    "            start = i\n",
    "            for dt in data:\n",
    "                t,c,ch,pos,l = dt\n",
    "                l = np.expand_dims(l,-1)\n",
    "                tokens.append(t)\n",
    "                char.append(ch)\n",
    "                labels.append(l)\n",
    "                casing.append(c)\n",
    "                pos_tags.append(pos)\n",
    "            yield np.asarray(labels),np.asarray(tokens),np.asarray(casing), np.asarray(char), np.asarray(pos_tags)\n",
    "    \n",
    "    def BiRNN_model(self):\n",
    "    \n",
    "        input = Input(shape=(None,),dtype='int32')\n",
    "\n",
    "        words = Embedding(input_dim=self.wd_em.shape[0], output_dim=self.wd_em.shape[1],  weights=[self.wd_em], trainable=False)(input)\n",
    "\n",
    "        csng_input = Input(shape=(None,), dtype='int32')\n",
    "        csng = Embedding(output_dim = self.caseEmbeddings.shape[1], input_dim = self.caseEmbeddings.shape[0], weights = [self.caseEmbeddings], trainable=False)(csng_input)\n",
    "\n",
    "\n",
    "        char_input=Input(shape=(None,52,))\n",
    "        embed_char_out=TimeDistributed(Embedding(len(self.ch_to_id),30,embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)))(char_input)\n",
    "        dropout= Dropout(self.params['dropout_rate'])(embed_char_out)\n",
    "        conv1d_out = TimeDistributed(Conv1D(kernel_size=self.params['kernel_sizes_cnn'], filters=30, padding='same',activation=params['rnn_activation'], strides=1))(dropout)\n",
    "        maxpool_out=TimeDistributed(MaxPooling1D(52))(conv1d_out)\n",
    "        char = TimeDistributed(Flatten())(maxpool_out)\n",
    "        char = Dropout(self.params['dropout_rate'])(char)\n",
    "\n",
    "        pos_input = Input(shape=(None,), dtype='int32')\n",
    "        pos = Embedding(output_dim = self.posEmbeddings.shape[1], input_dim = self.posEmbeddings.shape[0], weights = [self.posEmbeddings], trainable=False)(pos_input)\n",
    "\n",
    "\n",
    "        output = concatenate([words, csng, char, pos])\n",
    "        output = Bidirectional(LSTM(self.params['units_lstm'], return_sequences=True, dropout=self.params['dropout_rate'], recurrent_dropout=0.25))(output)\n",
    "        output = TimeDistributed(Dense(len(self.lb_to_id), activation=self.params['rnn_activation']))(output)\n",
    "        self.model = Model(inputs=[input, csng_input, char_input, pos_input], outputs=[output])\n",
    "        self.model.compile(loss=self.params['loss'], optimizer=self.params['optimizer'],metrics=[\"accuracy\"])\n",
    "\n",
    "    def train_model(self):\n",
    "    \n",
    "        for epoch in range(self.params['epochs']):\n",
    "\n",
    "            print(\"Epoch %d/%d\"%(epoch+1, self.params['epochs']))\n",
    "            a = Progbar(len(preprocess_obj.batch_len))\n",
    "            res = None\n",
    "            for i,batch in enumerate(self.iterate_minibatches(self.train_batch,self.train_batch_len)):\n",
    "                labels, tkns, csng, char, pos = batch       \n",
    "                res = self.model.train_on_batch([tkns, csng, char, pos], labels)\n",
    "                a.update(i)\n",
    "            print(\"\\n\")\n",
    "            print(self.model.metrics_names[0],\":\",res[0],self.model.metrics_names[1],\":\",res[1])\n",
    "            print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "            \"kernel_sizes_cnn\": 3,\n",
    "            \"optimizer\": \"nadam\",\n",
    "            \"cnn_activation\":\"tanh\",\n",
    "            \"rnn_activation\":\"softmax\",\n",
    "            \"units_lstm\" : 100,\n",
    "            \"loss\": \"sparse_categorical_crossentropy\",\n",
    "            \"text_size\": 50,\n",
    "            \"dropout_rate\": 0.5,\n",
    "            \"epochs\": 100,\n",
    "            \"model_name\": \"cnn_model\",\n",
    "            \"batch_size\": 32,\n",
    "            \"verbose\": True,\n",
    "            \"metrics\":[\"accuracy\"]\n",
    "        }\n",
    "model_obj = DesignModel(params)\n",
    "model_obj.BiRNN_model()\n",
    "model_obj.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadAndSaveModels():\n",
    "        \n",
    "    def save_model(self,model,model_name):\n",
    "        model.save(\"Model_Data/entity_models/\"+model_name+\".h5\")\n",
    "        print(\"Model saved to Model folder.\")\n",
    "        \n",
    "    def save_dict(self, save_path,dictionaries):  \n",
    "        \n",
    "        for item in dictionaries:\n",
    "            \n",
    "            with open(save_path+\"/\"+item[1]+\".txt\", \"wb\") as myFile:\n",
    "                pickle.dump(item[0], myFile)\n",
    "\n",
    "        print(\"Files saved.\")\n",
    "        \n",
    "    def load_dict(self,file):\n",
    "        with open(file,\"rb\") as fp:\n",
    "            dict = pickle.load(fp)\n",
    "        return dict\n",
    "    \n",
    "    def load_model(self,model_name):\n",
    "        model = load_model(model_name)\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_save = LoadAndSaveModels()\n",
    "load_save.save_model(model_obj.model,\"birnn\")\n",
    "dict = [(word_to_id,\"word_to_id\"),(label_to_id,\"label_to_id\"),(char_to_id,\"char_to_id\"),\n",
    "        (id_to_label,\"id_to_label\"),(case_to_id,\"case_to_id\"),(pos_to_id,\"pos_to_id\")]\n",
    "load_save.save_dict(\"Model_Data/dict\",dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_save = LoadAndSaveModels()\n",
    "model = load_save.load_model(\"Model_Data/entity_models/birnn.h5\")\n",
    "word_to_id = load_save.load_dict(\"Model_Data/dict/word_to_id.txt\")\n",
    "case_to_id = load_save.load_dict(\"Model_Data/dict/case_to_id.txt\")\n",
    "pos_to_id = load_save.load_dict(\"Model_Data/dict/pos_to_id.txt\")\n",
    "char_to_id = load_save.load_dict(\"Model_Data/dict/char_to_id.txt\")\n",
    "label_to_id = load_save.load_dict(\"Model_Data/dict/label_to_id.txt\")\n",
    "id_to_label = load_save.load_dict(\"Model_Data/dict/id_to_label.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_sentences = preprocess_obj.get_sentences(load_data_obj.validation_files)\n",
    "validation_set = preprocess_obj.padding(preprocess_obj.create_tensors(validation_sentences,word_to_id,case_to_id,pos_to_id,char_to_id,label_to_id))\n",
    "validation_batch,validation_batch_len = preprocess_obj.make_batch(validation_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction():\n",
    "    def __init__(self):\n",
    "        self.case_to_id = case_to_id\n",
    "        self.pos_to_id = pos_to_id\n",
    "        self.char_to_id = char_to_id\n",
    "        self.label_to_id = label_to_id\n",
    "        self.id_to_label = id_to_label\n",
    "        self.word_to_id = word_to_id\n",
    "    def prediction(self,dataset,model):\n",
    "        correct_labels = []\n",
    "        predict_labels = []\n",
    "        b = Progbar(len(dataset))\n",
    "        for i,data in enumerate(dataset):    \n",
    "            tkns, csng, char,pos, labels = data\n",
    "            tkns = np.asarray([tkns])     \n",
    "            char = np.asarray([char])\n",
    "            csng = np.asarray([csng])\n",
    "            pos = np.asarray([pos])\n",
    "            predict = model.predict([tkns, csng, char,pos], verbose=False)[0] \n",
    "            predict = predict.argmax(axis=-1)        \n",
    "            correct_labels.append(labels)\n",
    "            predict_labels.append(predict)\n",
    "            b.update(i)\n",
    "        return predict_labels, correct_labels\n",
    "    \n",
    "    def predict(self,sentence,model):\n",
    "        sen_list = [[[i,'POS','O\\n'] for i in sentence.split()]]\n",
    "        test_sent = preprocess_obj.addCharInformatioin(sen_list)\n",
    "\n",
    "        predLabels = []\n",
    "\n",
    "        test_set = preprocess_obj.padding(preprocess_obj.create_tensors(test_sent,self.word_to_id,\n",
    "                                                                        self.case_to_id,self.pos_to_id,\n",
    "                                                                        self.char_to_id,self.label_to_id))\n",
    "        test_batch,test_batch_len = preprocess_obj.createBatches(test_set)\n",
    "        for i,data in enumerate(test_batch):\n",
    "            tokens, csng, char, pos, labels = data\n",
    "            tokens = np.asarray([tokens])     \n",
    "            char = np.asarray([char])\n",
    "            csng = np.asarray([csng])\n",
    "            pos = np.asarray([pos])\n",
    "            pred = model.predict([tokens,csng, char,pos], verbose=False)[0] \n",
    "            pred = pred.argmax(axis=-1) #Predict the classes            \n",
    "            predLabels.append(pred)\n",
    "        entity_labels = []\n",
    "        j = 0\n",
    "        words_list = sentence.split()\n",
    "        for i in predLabels[-1]:\n",
    "            entity_labels.append((words_list[j],self.id_to_label[int(i)].replace(\"\\n\",\"\")))\n",
    "            j+=1\n",
    "\n",
    "        return entity_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_obj = Prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Add Richard McNamara newest song to the Just Smile playlist\"\n",
    "entity_label = pred_obj.predict(sent,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluate():\n",
    "    def compute_precision(self,guessed_sentences, correct_sentences):\n",
    "        assert(len(guessed_sentences) == len(correct_sentences))\n",
    "        correctCount = 0\n",
    "        count = 0\n",
    "\n",
    "\n",
    "        for sentenceIdx in range(len(guessed_sentences)):\n",
    "            guessed = guessed_sentences[sentenceIdx]\n",
    "            correct = correct_sentences[sentenceIdx]\n",
    "            assert(len(guessed) == len(correct))\n",
    "            idx = 0\n",
    "            while idx < len(guessed):\n",
    "                if guessed[idx][0] == 'B': #A new chunk starts\n",
    "                    count += 1\n",
    "\n",
    "                    if guessed[idx] == correct[idx]:\n",
    "                        idx += 1\n",
    "                        correctlyFound = True\n",
    "\n",
    "                        while idx < len(guessed) and guessed[idx][0] == 'I': #Scan until it no longer starts with I\n",
    "                            if guessed[idx] != correct[idx]:\n",
    "                                correctlyFound = False\n",
    "\n",
    "                            idx += 1\n",
    "\n",
    "                        if idx < len(guessed):\n",
    "                            if correct[idx][0] == 'I': #The chunk in correct was longer\n",
    "                                correctlyFound = False\n",
    "\n",
    "\n",
    "                        if correctlyFound:\n",
    "                            correctCount += 1\n",
    "                    else:\n",
    "                        idx += 1\n",
    "                else:  \n",
    "                    idx += 1\n",
    "\n",
    "        precision = 0\n",
    "        if count > 0:    \n",
    "            precision = float(correctCount) / count\n",
    "\n",
    "        return precision\n",
    "    def get_metrics(self,predictions, correct, idx2Label): \n",
    "        label_pred = []    \n",
    "        for sentence in predictions:\n",
    "            label_pred.append([idx2Label[element] for element in sentence])\n",
    "\n",
    "        label_correct = []    \n",
    "        for sentence in correct:\n",
    "            label_correct.append([idx2Label[element] for element in sentence])\n",
    "\n",
    "\n",
    "        #print label_pred\n",
    "        #print label_correct\n",
    "\n",
    "        prec = self.compute_precision(label_pred, label_correct)\n",
    "        rec = self.compute_precision(label_correct, label_pred)\n",
    "\n",
    "        f1 = 0\n",
    "        if (rec+prec) > 0:\n",
    "            f1 = 2.0 * prec * rec / (prec + rec);\n",
    "\n",
    "        return prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_obj = Evaluate()\n",
    "\n",
    "train_predict_labels, train_correct_labels = pred_obj.prediction(train_data_set,model)\n",
    "pre_train, rec_train, f1_train= eval_obj.get_metrics(train_predict_labels, train_correct_labels, id_to_label)\n",
    "print(\"Train-Data: Precision: %.3f, Recall: %.3f, F1 Score: %.3f\" % (pre_train, rec_train, f1_train))\n",
    "     \n",
    "validation_predict_labels, validation_correct_labels = pred_obj.prediction(validation_set,model)\n",
    "pre_test, rec_test, f1_test= eval_obj.get_metrics(validation_predict_labels, validation_correct_labels, id_to_label)\n",
    "print(\"Validation-Data: Precision: %.3f, Recall: %.3f, F1 Score: %.3f\" % (pre_test, rec_test, f1_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
